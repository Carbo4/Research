Train datasets: 304; Val datasets: 181
Available Samples: 53773; Batch Size: 64
Available Samples: 1627; Batch Size: 64
Epoch 1 batch 50: loss=-0.486423 grad_norm=14.8148 lr=1.000000e-04 gate_mean=0.4960 gate_entropy=0.6929
Epoch 1 batch 100: loss=-3.221106 grad_norm=13.2566 lr=1.000000e-04 gate_mean=0.4963 gate_entropy=0.6929
Epoch 1 batch 150: loss=-3.792165 grad_norm=113.6011 lr=1.000000e-04 gate_mean=0.5002 gate_entropy=0.6928
Epoch 1 batch 200: loss=-6.745153 grad_norm=19.5693 lr=1.000000e-04 gate_mean=0.5005 gate_entropy=0.6928
Epoch 1 batch 250: loss=-6.671955 grad_norm=36.3665 lr=1.000000e-04 gate_mean=0.4988 gate_entropy=0.6928
Epoch 1 batch 300: loss=-7.978759 grad_norm=116.9650 lr=1.000000e-04 gate_mean=0.4990 gate_entropy=0.6928
Epoch 1 batch 350: loss=-6.303137 grad_norm=71.4668 lr=1.000000e-04 gate_mean=0.5004 gate_entropy=0.6929
Epoch 1 batch 400: loss=-9.021562 grad_norm=41.4658 lr=1.000000e-04 gate_mean=0.4988 gate_entropy=0.6929
Epoch 1 batch 450: loss=-9.085162 grad_norm=95.9932 lr=1.000000e-04 gate_mean=0.5006 gate_entropy=0.6928
Epoch 1 batch 500: loss=-7.285060 grad_norm=40.3070 lr=1.000000e-04 gate_mean=0.4995 gate_entropy=0.6929
Epoch 1 batch 550: loss=-1.905758 grad_norm=32.9911 lr=1.000000e-04 gate_mean=0.5030 gate_entropy=0.6928
Epoch 1 batch 600: loss=-7.926303 grad_norm=124.4921 lr=1.000000e-04 gate_mean=0.5005 gate_entropy=0.6929
Epoch 1 batch 650: loss=3.744669 grad_norm=1312.9464 lr=1.000000e-04 gate_mean=0.4978 gate_entropy=0.6928
Epoch 1 batch 700: loss=-0.512306 grad_norm=28.6675 lr=1.000000e-04 gate_mean=0.5004 gate_entropy=0.6929
Epoch 1 batch 750: loss=-1.858137 grad_norm=61.8727 lr=1.000000e-04 gate_mean=0.4974 gate_entropy=0.6928
Epoch 1 batch 800: loss=-6.014946 grad_norm=35.7902 lr=1.000000e-04 gate_mean=0.4982 gate_entropy=0.6928
Epoch 1/15 train_loss=-4.525851 val_loss=-4.107705 gate_mean=0.4994 gate_std=0.0016 gate_entropy=0.6928 val_mae_close=0.24083726108074188
Epoch 2 batch 50: loss=-8.139768 grad_norm=49.1150 lr=1.000000e-04 gate_mean=0.4990 gate_entropy=0.6929
Epoch 2 batch 100: loss=-8.360165 grad_norm=157.4951 lr=1.000000e-04 gate_mean=0.4988 gate_entropy=0.6929
Epoch 2 batch 150: loss=-7.932117 grad_norm=61.1192 lr=1.000000e-04 gate_mean=0.5004 gate_entropy=0.6929
Epoch 2 batch 200: loss=-9.943460 grad_norm=34.0053 lr=1.000000e-04 gate_mean=0.4992 gate_entropy=0.6928
Epoch 2 batch 250: loss=-9.782084 grad_norm=37.2529 lr=1.000000e-04 gate_mean=0.4969 gate_entropy=0.6928
Epoch 2 batch 300: loss=-8.513666 grad_norm=50.9034 lr=1.000000e-04 gate_mean=0.5008 gate_entropy=0.6929
Epoch 2 batch 350: loss=-8.282447 grad_norm=35.3408 lr=1.000000e-04 gate_mean=0.4975 gate_entropy=0.6929
Epoch 2 batch 400: loss=-9.681954 grad_norm=35.2240 lr=1.000000e-04 gate_mean=0.4972 gate_entropy=0.6929
Epoch 2 batch 450: loss=-8.400651 grad_norm=39.8902 lr=1.000000e-04 gate_mean=0.5005 gate_entropy=0.6929
Epoch 2 batch 500: loss=-0.312792 grad_norm=43.9826 lr=1.000000e-04 gate_mean=0.5022 gate_entropy=0.6929
Epoch 2 batch 550: loss=-1.872182 grad_norm=23.1581 lr=1.000000e-04 gate_mean=0.4996 gate_entropy=0.6929
Epoch 2 batch 600: loss=-4.879154 grad_norm=34.4327 lr=1.000000e-04 gate_mean=0.4991 gate_entropy=0.6929
Epoch 2 batch 650: loss=-5.584864 grad_norm=67.7554 lr=1.000000e-04 gate_mean=0.5001 gate_entropy=0.6929
Epoch 2 batch 700: loss=-7.270258 grad_norm=35.9369 lr=1.000000e-04 gate_mean=0.5008 gate_entropy=0.6929
Epoch 2 batch 750: loss=-2.951580 grad_norm=287.3395 lr=1.000000e-04 gate_mean=0.4993 gate_entropy=0.6928
Epoch 2 batch 800: loss=9.601562 grad_norm=113.6454 lr=1.000000e-04 gate_mean=0.5033 gate_entropy=0.6929
Epoch 2/15 train_loss=-4.852844 val_loss=4.609960 gate_mean=0.5001 gate_std=0.0015 gate_entropy=0.6929 val_mae_close=1.3649873733520508
Epoch 3 batch 50: loss=-0.217815 grad_norm=20.7962 lr=1.000000e-04 gate_mean=0.5018 gate_entropy=0.6929
Epoch 3 batch 100: loss=-1.843324 grad_norm=20.6197 lr=1.000000e-04 gate_mean=0.5022 gate_entropy=0.6930
Epoch 3 batch 150: loss=-2.799421 grad_norm=27.2889 lr=1.000000e-04 gate_mean=0.5019 gate_entropy=0.6930
Epoch 3 batch 200: loss=-3.670481 grad_norm=35.2026 lr=1.000000e-04 gate_mean=0.5001 gate_entropy=0.6929
Epoch 3 batch 250: loss=-4.363350 grad_norm=47.5478 lr=1.000000e-04 gate_mean=0.4981 gate_entropy=0.6930
Epoch 3 batch 300: loss=-5.920450 grad_norm=31.3823 lr=1.000000e-04 gate_mean=0.5016 gate_entropy=0.6930
Epoch 3 batch 350: loss=-5.036795 grad_norm=68.4151 lr=1.000000e-04 gate_mean=0.5016 gate_entropy=0.6929
Epoch 3 batch 400: loss=-6.062746 grad_norm=35.5320 lr=1.000000e-04 gate_mean=0.5013 gate_entropy=0.6929
Epoch 3 batch 450: loss=-5.032146 grad_norm=54.3376 lr=1.000000e-04 gate_mean=0.5014 gate_entropy=0.6929
Epoch 3 batch 500: loss=-6.823074 grad_norm=32.7499 lr=1.000000e-04 gate_mean=0.5025 gate_entropy=0.6929
Epoch 3 batch 550: loss=-8.833298 grad_norm=35.1403 lr=1.000000e-04 gate_mean=0.5019 gate_entropy=0.6929
Epoch 3 batch 600: loss=-9.416419 grad_norm=38.1402 lr=1.000000e-04 gate_mean=0.5017 gate_entropy=0.6929
Epoch 3 batch 650: loss=-7.125376 grad_norm=44.4327 lr=1.000000e-04 gate_mean=0.5003 gate_entropy=0.6929
Epoch 3 batch 700: loss=-8.487809 grad_norm=35.6516 lr=1.000000e-04 gate_mean=0.5006 gate_entropy=0.6929
Epoch 3 batch 750: loss=-8.899782 grad_norm=40.2189 lr=1.000000e-04 gate_mean=0.5020 gate_entropy=0.6930
Epoch 3 batch 800: loss=-7.167738 grad_norm=46.9566 lr=1.000000e-04 gate_mean=0.5014 gate_entropy=0.6930
Epoch 3/15 train_loss=-4.833036 val_loss=-2.110494 gate_mean=0.5011 gate_std=0.0013 gate_entropy=0.6929 val_mae_close=0.6932312846183777
Epoch 4 batch 50: loss=3.661436 grad_norm=53.8779 lr=1.000000e-04 gate_mean=0.5005 gate_entropy=0.6930
Epoch 4 batch 100: loss=2.016617 grad_norm=32.0739 lr=1.000000e-04 gate_mean=0.5021 gate_entropy=0.6930
Epoch 4 batch 150: loss=0.593475 grad_norm=33.0037 lr=1.000000e-04 gate_mean=0.5002 gate_entropy=0.6930
Epoch 4 batch 200: loss=-1.214997 grad_norm=25.3565 lr=1.000000e-04 gate_mean=0.5003 gate_entropy=0.6930
Epoch 4 batch 250: loss=-1.165858 grad_norm=33.8755 lr=1.000000e-04 gate_mean=0.5011 gate_entropy=0.6929
Epoch 4 batch 300: loss=-3.038625 grad_norm=34.7802 lr=1.000000e-04 gate_mean=0.5009 gate_entropy=0.6929
Epoch 4 batch 350: loss=-5.173441 grad_norm=37.4298 lr=1.000000e-04 gate_mean=0.5018 gate_entropy=0.6930
Epoch 4 batch 400: loss=-5.450418 grad_norm=35.7486 lr=1.000000e-04 gate_mean=0.4998 gate_entropy=0.6930
Epoch 4 batch 450: loss=-5.201421 grad_norm=45.8059 lr=1.000000e-04 gate_mean=0.5005 gate_entropy=0.6930
Epoch 4 batch 500: loss=-4.391972 grad_norm=53.0397 lr=1.000000e-04 gate_mean=0.5002 gate_entropy=0.6930
Epoch 4 batch 550: loss=-4.344396 grad_norm=43.8915 lr=1.000000e-04 gate_mean=0.5028 gate_entropy=0.6930
Epoch 4 batch 600: loss=-3.065436 grad_norm=93.1173 lr=1.000000e-04 gate_mean=0.5010 gate_entropy=0.6930
Epoch 4 batch 650: loss=-5.379313 grad_norm=34.5489 lr=1.000000e-04 gate_mean=0.5007 gate_entropy=0.6930
Epoch 4 batch 700: loss=-6.874948 grad_norm=33.5763 lr=1.000000e-04 gate_mean=0.5010 gate_entropy=0.6930
Epoch 4 batch 750: loss=-7.165202 grad_norm=38.6208 lr=1.000000e-04 gate_mean=0.5033 gate_entropy=0.6930
Epoch 4 batch 800: loss=-7.209242 grad_norm=40.4739 lr=1.000000e-04 gate_mean=0.5012 gate_entropy=0.6930
Epoch 4/15 train_loss=0.764280 val_loss=-5.680820 gate_mean=0.5011 gate_std=0.0012 gate_entropy=0.6930 val_mae_close=0.6542269587516785
Epoch 5 batch 50: loss=-6.904468 grad_norm=152.8536 lr=1.000000e-04 gate_mean=0.5029 gate_entropy=0.6930
Epoch 5 batch 100: loss=-5.800866 grad_norm=57.4930 lr=1.000000e-04 gate_mean=0.5023 gate_entropy=0.6930
Epoch 5 batch 150: loss=-6.843561 grad_norm=40.1911 lr=1.000000e-04 gate_mean=0.4992 gate_entropy=0.6930
Epoch 5 batch 200: loss=43.431229 grad_norm=6909.1536 lr=1.000000e-04 gate_mean=0.5021 gate_entropy=0.6930
Epoch 5 batch 250: loss=-6.397773 grad_norm=37.0126 lr=1.000000e-04 gate_mean=0.5009 gate_entropy=0.6930
Epoch 5 batch 300: loss=-5.215558 grad_norm=46.8090 lr=1.000000e-04 gate_mean=0.5031 gate_entropy=0.6930
Epoch 5 batch 350: loss=-7.567896 grad_norm=39.2764 lr=1.000000e-04 gate_mean=0.5029 gate_entropy=0.6930
Epoch 5 batch 400: loss=-3.319849 grad_norm=59.1794 lr=1.000000e-04 gate_mean=0.5010 gate_entropy=0.6930
Epoch 5 batch 450: loss=-5.753775 grad_norm=45.5505 lr=1.000000e-04 gate_mean=0.5013 gate_entropy=0.6930
Epoch 5 batch 500: loss=-6.406087 grad_norm=44.0378 lr=1.000000e-04 gate_mean=0.5017 gate_entropy=0.6930
Epoch 5 batch 550: loss=-6.052804 grad_norm=41.4210 lr=1.000000e-04 gate_mean=0.4996 gate_entropy=0.6930
Epoch 5 batch 600: loss=-8.386521 grad_norm=47.5662 lr=1.000000e-04 gate_mean=0.5013 gate_entropy=0.6930
Epoch 5 batch 650: loss=-10.212240 grad_norm=44.4275 lr=1.000000e-04 gate_mean=0.5014 gate_entropy=0.6930
Epoch 5 batch 700: loss=-9.673717 grad_norm=72.2623 lr=1.000000e-04 gate_mean=0.5016 gate_entropy=0.6930
Epoch 5 batch 750: loss=-10.027369 grad_norm=67.5508 lr=1.000000e-04 gate_mean=0.5009 gate_entropy=0.6930
Epoch 5 batch 800: loss=-0.687842 grad_norm=27.9100 lr=1.000000e-04 gate_mean=0.5002 gate_entropy=0.6930
Epoch 5/15 train_loss=-5.157876 val_loss=41.361596 gate_mean=0.5011 gate_std=0.0011 gate_entropy=0.6930 val_mae_close=0.9051436185836792
Epoch 6 batch 50: loss=-2.573487 grad_norm=25.4691 lr=1.000000e-04 gate_mean=0.5004 gate_entropy=0.6930
Epoch 6 batch 100: loss=-3.627655 grad_norm=26.8104 lr=1.000000e-04 gate_mean=0.5017 gate_entropy=0.6931
Epoch 6 batch 150: loss=-5.108697 grad_norm=26.2806 lr=1.000000e-04 gate_mean=0.5013 gate_entropy=0.6930
Epoch 6 batch 200: loss=-2.264466 grad_norm=122.3725 lr=1.000000e-04 gate_mean=0.5024 gate_entropy=0.6931
Epoch 6 batch 250: loss=-2.567027 grad_norm=44.1627 lr=1.000000e-04 gate_mean=0.5021 gate_entropy=0.6930
Epoch 6 batch 300: loss=-2.562003 grad_norm=35.0868 lr=1.000000e-04 gate_mean=0.5006 gate_entropy=0.6930
Epoch 6 batch 350: loss=-3.321119 grad_norm=35.0240 lr=1.000000e-04 gate_mean=0.5000 gate_entropy=0.6930
Epoch 6 batch 400: loss=-4.539433 grad_norm=34.6952 lr=1.000000e-04 gate_mean=0.4999 gate_entropy=0.6931
Epoch 6 batch 450: loss=-4.600503 grad_norm=29.7097 lr=1.000000e-04 gate_mean=0.4993 gate_entropy=0.6930
Epoch 6 batch 500: loss=-3.362936 grad_norm=90.4199 lr=1.000000e-04 gate_mean=0.5003 gate_entropy=0.6930
Epoch 6 batch 550: loss=-5.962349 grad_norm=27.1665 lr=1.000000e-04 gate_mean=0.5015 gate_entropy=0.6931
Epoch 6 batch 600: loss=-5.536887 grad_norm=49.4120 lr=1.000000e-04 gate_mean=0.5004 gate_entropy=0.6930
Epoch 6 batch 650: loss=-6.247233 grad_norm=28.0733 lr=1.000000e-04 gate_mean=0.4991 gate_entropy=0.6931
Epoch 6 batch 700: loss=-5.601024 grad_norm=108.9750 lr=1.000000e-04 gate_mean=0.5020 gate_entropy=0.6930
Epoch 6 batch 750: loss=-7.857813 grad_norm=28.6602 lr=1.000000e-04 gate_mean=0.5022 gate_entropy=0.6930
Epoch 6 batch 800: loss=2.986887 grad_norm=141.8220 lr=1.000000e-04 gate_mean=0.5007 gate_entropy=0.6931
Epoch 6/15 train_loss=35.252536 val_loss=4.799581 gate_mean=0.5008 gate_std=0.0009 gate_entropy=0.6930 val_mae_close=1.1795382499694824
Epoch 7 batch 50: loss=-0.548086 grad_norm=61.0194 lr=1.000000e-04 gate_mean=0.5001 gate_entropy=0.6931
Epoch 7 batch 100: loss=-2.165437 grad_norm=46.0425 lr=1.000000e-04 gate_mean=0.5005 gate_entropy=0.6931
Epoch 7 batch 150: loss=-2.795906 grad_norm=35.0817 lr=1.000000e-04 gate_mean=0.5033 gate_entropy=0.6930
Epoch 7 batch 200: loss=-2.949228 grad_norm=43.9785 lr=1.000000e-04 gate_mean=0.5008 gate_entropy=0.6931
Epoch 7 batch 250: loss=-2.406320 grad_norm=34.0169 lr=1.000000e-04 gate_mean=0.5026 gate_entropy=0.6930
Epoch 7 batch 300: loss=-2.190887 grad_norm=53.5235 lr=1.000000e-04 gate_mean=0.5019 gate_entropy=0.6931
Epoch 7 batch 350: loss=-4.047350 grad_norm=26.3220 lr=1.000000e-04 gate_mean=0.5007 gate_entropy=0.6931
Epoch 7 batch 400: loss=-4.748909 grad_norm=23.0357 lr=1.000000e-04 gate_mean=0.5020 gate_entropy=0.6930
Epoch 7 batch 450: loss=-4.245328 grad_norm=31.7116 lr=1.000000e-04 gate_mean=0.5010 gate_entropy=0.6931
Epoch 7 batch 500: loss=-4.655906 grad_norm=54.5941 lr=1.000000e-04 gate_mean=0.5014 gate_entropy=0.6931
Epoch 7 batch 550: loss=21.894255 grad_norm=1415.7017 lr=1.000000e-04 gate_mean=0.5014 gate_entropy=0.6931
Epoch 7 batch 600: loss=-4.682222 grad_norm=78.5167 lr=1.000000e-04 gate_mean=0.5006 gate_entropy=0.6931
Epoch 7 batch 650: loss=-4.921779 grad_norm=152.3417 lr=1.000000e-04 gate_mean=0.5016 gate_entropy=0.6931
Epoch 7 batch 700: loss=-5.383639 grad_norm=59.3590 lr=1.000000e-04 gate_mean=0.5009 gate_entropy=0.6931
Epoch 7 batch 750: loss=8.628720 grad_norm=1039.1438 lr=1.000000e-04 gate_mean=0.5016 gate_entropy=0.6931
Epoch 7 batch 800: loss=-4.478131 grad_norm=83.2452 lr=1.000000e-04 gate_mean=0.5023 gate_entropy=0.6931
Epoch 7/15 train_loss=-3.352713 val_loss=-2.024154 gate_mean=0.5013 gate_std=0.0008 gate_entropy=0.6931 val_mae_close=0.5335878133773804
Epoch 8 batch 50: loss=-4.322749 grad_norm=36.6525 lr=1.000000e-04 gate_mean=0.4999 gate_entropy=0.6931
Epoch 8 batch 100: loss=-5.802004 grad_norm=43.4830 lr=1.000000e-04 gate_mean=0.5011 gate_entropy=0.6931
Epoch 8 batch 150: loss=-5.591595 grad_norm=87.3205 lr=1.000000e-04 gate_mean=0.5026 gate_entropy=0.6931
Epoch 8 batch 200: loss=-3.461868 grad_norm=178.6553 lr=1.000000e-04 gate_mean=0.5017 gate_entropy=0.6931
Epoch 8 batch 250: loss=-6.300992 grad_norm=36.2403 lr=1.000000e-04 gate_mean=0.5012 gate_entropy=0.6931
Epoch 8 batch 300: loss=-6.152041 grad_norm=54.1870 lr=1.000000e-04 gate_mean=0.5003 gate_entropy=0.6931
Epoch 8 batch 350: loss=-5.752923 grad_norm=187.8983 lr=1.000000e-04 gate_mean=0.5014 gate_entropy=0.6931
Epoch 8 batch 400: loss=-4.536131 grad_norm=93.4624 lr=1.000000e-04 gate_mean=0.5009 gate_entropy=0.6931
Epoch 8 batch 450: loss=-6.497766 grad_norm=47.5222 lr=1.000000e-04 gate_mean=0.5019 gate_entropy=0.6931
Epoch 8 batch 500: loss=-5.936857 grad_norm=38.4135 lr=1.000000e-04 gate_mean=0.5011 gate_entropy=0.6931
Epoch 8 batch 550: loss=-6.733983 grad_norm=66.2603 lr=1.000000e-04 gate_mean=0.5012 gate_entropy=0.6931
Epoch 8 batch 600: loss=-5.873702 grad_norm=101.2443 lr=1.000000e-04 gate_mean=0.5023 gate_entropy=0.6931
Epoch 8 batch 650: loss=-6.332924 grad_norm=89.5123 lr=1.000000e-04 gate_mean=0.5017 gate_entropy=0.6931
Epoch 8 batch 700: loss=-7.240421 grad_norm=74.2714 lr=1.000000e-04 gate_mean=0.5010 gate_entropy=0.6931
Epoch 8 batch 750: loss=-7.549499 grad_norm=53.6326 lr=1.000000e-04 gate_mean=0.5011 gate_entropy=0.6931
Epoch 8 batch 800: loss=-6.804100 grad_norm=65.4691 lr=1.000000e-04 gate_mean=0.5010 gate_entropy=0.6931
Epoch 8/15 train_loss=-5.643556 val_loss=-3.104851 gate_mean=0.5013 gate_std=0.0007 gate_entropy=0.6931 val_mae_close=0.3571755886077881
Epoch 9 batch 50: loss=-7.443347 grad_norm=54.1320 lr=1.000000e-04 gate_mean=0.5014 gate_entropy=0.6931
Epoch 9 batch 100: loss=-7.105987 grad_norm=148.5646 lr=1.000000e-04 gate_mean=0.5011 gate_entropy=0.6931
Epoch 9 batch 150: loss=-6.826678 grad_norm=134.7918 lr=1.000000e-04 gate_mean=0.4995 gate_entropy=0.6931
Epoch 9 batch 200: loss=-7.380590 grad_norm=106.7287 lr=1.000000e-04 gate_mean=0.5010 gate_entropy=0.6931
Epoch 9 batch 250: loss=-8.254874 grad_norm=48.4973 lr=1.000000e-04 gate_mean=0.5013 gate_entropy=0.6931
Epoch 9 batch 300: loss=-7.245987 grad_norm=65.3060 lr=1.000000e-04 gate_mean=0.5012 gate_entropy=0.6931
Epoch 9 batch 350: loss=-7.233975 grad_norm=63.1321 lr=1.000000e-04 gate_mean=0.5021 gate_entropy=0.6931
Epoch 9 batch 400: loss=-7.544960 grad_norm=80.1454 lr=1.000000e-04 gate_mean=0.5006 gate_entropy=0.6931
Epoch 9 batch 450: loss=-6.228214 grad_norm=65.0877 lr=1.000000e-04 gate_mean=0.5015 gate_entropy=0.6931
Epoch 9 batch 500: loss=-7.383999 grad_norm=67.3113 lr=1.000000e-04 gate_mean=0.5004 gate_entropy=0.6931
Epoch 9 batch 550: loss=-8.354240 grad_norm=58.9611 lr=1.000000e-04 gate_mean=0.5007 gate_entropy=0.6931
Epoch 9 batch 600: loss=-8.215367 grad_norm=69.2462 lr=1.000000e-04 gate_mean=0.4999 gate_entropy=0.6931
Epoch 9 batch 650: loss=-7.203912 grad_norm=193.7976 lr=1.000000e-04 gate_mean=0.5017 gate_entropy=0.6931
Epoch 9 batch 700: loss=-0.505020 grad_norm=1323.8530 lr=1.000000e-04 gate_mean=0.5012 gate_entropy=0.6931
Epoch 9 batch 750: loss=-5.022856 grad_norm=421.4566 lr=1.000000e-04 gate_mean=0.5020 gate_entropy=0.6931
Epoch 9 batch 800: loss=-7.492183 grad_norm=47.5890 lr=1.000000e-04 gate_mean=0.5017 gate_entropy=0.6931
Epoch 9/15 train_loss=-6.664981 val_loss=-3.987369 gate_mean=0.5013 gate_std=0.0006 gate_entropy=0.6931 val_mae_close=0.31736862659454346
Epoch 10 batch 50: loss=-6.678825 grad_norm=108.7988 lr=1.000000e-04 gate_mean=0.5018 gate_entropy=0.6931
Epoch 10 batch 100: loss=-6.992967 grad_norm=208.0182 lr=1.000000e-04 gate_mean=0.5013 gate_entropy=0.6931
Epoch 10 batch 150: loss=-7.814030 grad_norm=284.3922 lr=1.000000e-04 gate_mean=0.5009 gate_entropy=0.6931
Epoch 10 batch 200: loss=-8.487097 grad_norm=117.5972 lr=1.000000e-04 gate_mean=0.5017 gate_entropy=0.6931
Epoch 10 batch 250: loss=-8.648189 grad_norm=221.4235 lr=1.000000e-04 gate_mean=0.5010 gate_entropy=0.6931
Epoch 10 batch 300: loss=-7.273597 grad_norm=387.9163 lr=1.000000e-04 gate_mean=0.5010 gate_entropy=0.6931
Epoch 10 batch 350: loss=-5.573105 grad_norm=132.2484 lr=1.000000e-04 gate_mean=0.5016 gate_entropy=0.6931
Epoch 10 batch 400: loss=-6.316986 grad_norm=36.3490 lr=1.000000e-04 gate_mean=0.5004 gate_entropy=0.6931
Epoch 10 batch 450: loss=-6.997582 grad_norm=109.1002 lr=1.000000e-04 gate_mean=0.5016 gate_entropy=0.6931
Epoch 10 batch 500: loss=-7.854199 grad_norm=50.6092 lr=1.000000e-04 gate_mean=0.5011 gate_entropy=0.6931
Epoch 10 batch 550: loss=-8.688763 grad_norm=48.9264 lr=1.000000e-04 gate_mean=0.5016 gate_entropy=0.6931
Epoch 10 batch 600: loss=-8.108849 grad_norm=413.5488 lr=1.000000e-04 gate_mean=0.5018 gate_entropy=0.6931
Epoch 10 batch 650: loss=-7.411128 grad_norm=105.6681 lr=1.000000e-04 gate_mean=0.5016 gate_entropy=0.6931
Epoch 10 batch 700: loss=-7.847077 grad_norm=172.1108 lr=1.000000e-04 gate_mean=0.5015 gate_entropy=0.6931
Epoch 10 batch 750: loss=-9.177801 grad_norm=85.0188 lr=1.000000e-04 gate_mean=0.5007 gate_entropy=0.6931
Epoch 10 batch 800: loss=-4.042247 grad_norm=58.6019 lr=1.000000e-04 gate_mean=0.5011 gate_entropy=0.6931
Epoch 10/15 train_loss=-6.802055 val_loss=-4.032122 gate_mean=0.5013 gate_std=0.0006 gate_entropy=0.6931 val_mae_close=0.5687139630317688
Epoch 11 batch 50: loss=-1.346020 grad_norm=250.4108 lr=1.000000e-04 gate_mean=0.5008 gate_entropy=0.6931
Epoch 11 batch 100: loss=-7.268087 grad_norm=44.5961 lr=1.000000e-04 gate_mean=0.5017 gate_entropy=0.6931
Epoch 11 batch 150: loss=-1.652365 grad_norm=619.5433 lr=1.000000e-04 gate_mean=0.5006 gate_entropy=0.6931
Epoch 11 batch 200: loss=-8.119778 grad_norm=54.3501 lr=1.000000e-04 gate_mean=0.5000 gate_entropy=0.6931
Epoch 11 batch 250: loss=-7.417964 grad_norm=59.9393 lr=1.000000e-04 gate_mean=0.5010 gate_entropy=0.6931
Epoch 11 batch 300: loss=-8.599607 grad_norm=54.5333 lr=1.000000e-04 gate_mean=0.5020 gate_entropy=0.6931
Epoch 11 batch 350: loss=-9.037833 grad_norm=46.7915 lr=1.000000e-04 gate_mean=0.5009 gate_entropy=0.6931
Epoch 11 batch 400: loss=-8.127073 grad_norm=125.3552 lr=1.000000e-04 gate_mean=0.5020 gate_entropy=0.6931
Epoch 11 batch 450: loss=-9.059848 grad_norm=65.7542 lr=1.000000e-04 gate_mean=0.5018 gate_entropy=0.6931
Epoch 11 batch 500: loss=-9.355065 grad_norm=51.1571 lr=1.000000e-04 gate_mean=0.5016 gate_entropy=0.6931
Epoch 11 batch 550: loss=-9.063464 grad_norm=193.5134 lr=1.000000e-04 gate_mean=0.5009 gate_entropy=0.6931
Epoch 11 batch 600: loss=-9.207458 grad_norm=88.3144 lr=1.000000e-04 gate_mean=0.5016 gate_entropy=0.6931
Epoch 11 batch 650: loss=-7.705508 grad_norm=169.9170 lr=1.000000e-04 gate_mean=0.5021 gate_entropy=0.6931
Epoch 11 batch 700: loss=-9.362528 grad_norm=108.9538 lr=1.000000e-04 gate_mean=0.5007 gate_entropy=0.6931
Epoch 11 batch 750: loss=-9.655148 grad_norm=53.8044 lr=1.000000e-04 gate_mean=0.5011 gate_entropy=0.6931
Epoch 11 batch 800: loss=-1.354631 grad_norm=61.6027 lr=1.000000e-04 gate_mean=0.5011 gate_entropy=0.6931
Epoch 11/15 train_loss=-6.714448 val_loss=-1.795111 gate_mean=0.5013 gate_std=0.0005 gate_entropy=0.6931 val_mae_close=0.5764800906181335
Epoch 12 batch 50: loss=-3.635267 grad_norm=54.9428 lr=1.000000e-04 gate_mean=0.5008 gate_entropy=0.6931
Epoch 12 batch 100: loss=-4.361401 grad_norm=48.8567 lr=1.000000e-04 gate_mean=0.5015 gate_entropy=0.6931
Epoch 12 batch 150: loss=-6.692418 grad_norm=37.8728 lr=1.000000e-04 gate_mean=0.5018 gate_entropy=0.6931
Epoch 12 batch 200: loss=-6.021441 grad_norm=290.4560 lr=1.000000e-04 gate_mean=0.5003 gate_entropy=0.6931
Epoch 12 batch 250: loss=-7.989764 grad_norm=106.7181 lr=1.000000e-04 gate_mean=0.5018 gate_entropy=0.6931
Epoch 12 batch 300: loss=-7.796749 grad_norm=109.5335 lr=1.000000e-04 gate_mean=0.5001 gate_entropy=0.6931
Epoch 12 batch 350: loss=-9.796252 grad_norm=154.0745 lr=1.000000e-04 gate_mean=0.5005 gate_entropy=0.6931
Epoch 12 batch 400: loss=-9.659719 grad_norm=67.8471 lr=1.000000e-04 gate_mean=0.5012 gate_entropy=0.6931
Epoch 12 batch 450: loss=-9.859065 grad_norm=102.5967 lr=1.000000e-04 gate_mean=0.5016 gate_entropy=0.6931
Epoch 12 batch 500: loss=-9.337625 grad_norm=131.3909 lr=1.000000e-04 gate_mean=0.5011 gate_entropy=0.6931
Epoch 12 batch 550: loss=-8.379375 grad_norm=173.3809 lr=1.000000e-04 gate_mean=0.5012 gate_entropy=0.6931
Epoch 12 batch 600: loss=-10.777305 grad_norm=76.5976 lr=1.000000e-04 gate_mean=0.5020 gate_entropy=0.6931
Epoch 12 batch 650: loss=-7.880911 grad_norm=131.5356 lr=1.000000e-04 gate_mean=0.5017 gate_entropy=0.6931
Epoch 12 batch 700: loss=-8.615361 grad_norm=58.1790 lr=1.000000e-04 gate_mean=0.5015 gate_entropy=0.6931
Epoch 12 batch 750: loss=-8.260422 grad_norm=188.9125 lr=1.000000e-04 gate_mean=0.5012 gate_entropy=0.6931
Epoch 12 batch 800: loss=-10.162122 grad_norm=57.1211 lr=1.000000e-04 gate_mean=0.5020 gate_entropy=0.6931
Epoch 12/15 train_loss=-7.571598 val_loss=-5.377000 gate_mean=0.5013 gate_std=0.0005 gate_entropy=0.6931 val_mae_close=0.2617812752723694
Epoch 13 batch 50: loss=-8.441295 grad_norm=55.2838 lr=1.000000e-04 gate_mean=0.5015 gate_entropy=0.6931
Epoch 13 batch 100: loss=22.268965 grad_norm=1859.8534 lr=1.000000e-04 gate_mean=0.5009 gate_entropy=0.6931
Epoch 13 batch 150: loss=-10.445809 grad_norm=50.7321 lr=1.000000e-04 gate_mean=0.5014 gate_entropy=0.6931
Epoch 13 batch 200: loss=-4.376861 grad_norm=176.5740 lr=1.000000e-04 gate_mean=0.5017 gate_entropy=0.6931
Epoch 13 batch 250: loss=-2.751201 grad_norm=62.1723 lr=1.000000e-04 gate_mean=0.5003 gate_entropy=0.6931
Epoch 13 batch 300: loss=-3.774641 grad_norm=56.7877 lr=1.000000e-04 gate_mean=0.5013 gate_entropy=0.6931
Epoch 13 batch 350: loss=-5.923408 grad_norm=55.9709 lr=1.000000e-04 gate_mean=0.5019 gate_entropy=0.6931
Epoch 13 batch 400: loss=-7.393268 grad_norm=51.5390 lr=1.000000e-04 gate_mean=0.5010 gate_entropy=0.6931
Epoch 13 batch 450: loss=-8.319633 grad_norm=51.5611 lr=1.000000e-04 gate_mean=0.5009 gate_entropy=0.6931
Epoch 13 batch 500: loss=-9.382099 grad_norm=49.5036 lr=1.000000e-04 gate_mean=0.5016 gate_entropy=0.6931
Epoch 13 batch 550: loss=-9.110290 grad_norm=113.6955 lr=1.000000e-04 gate_mean=0.5010 gate_entropy=0.6931
Epoch 13 batch 600: loss=-10.909777 grad_norm=46.9212 lr=1.000000e-04 gate_mean=0.5015 gate_entropy=0.6931
Epoch 13 batch 650: loss=3.755771 grad_norm=3257.9876 lr=1.000000e-04 gate_mean=0.5013 gate_entropy=0.6931
Epoch 13 batch 700: loss=3.675213 grad_norm=196.8419 lr=1.000000e-04 gate_mean=0.5022 gate_entropy=0.6931
Epoch 13 batch 750: loss=7.257662 grad_norm=134.7249 lr=1.000000e-04 gate_mean=0.5013 gate_entropy=0.6931
Epoch 13 batch 800: loss=6.294368 grad_norm=45.2458 lr=1.000000e-04 gate_mean=0.5004 gate_entropy=0.6931
Epoch 13/15 train_loss=114.795558 val_loss=7.559283 gate_mean=0.5013 gate_std=0.0004 gate_entropy=0.6931 val_mae_close=0.8337777256965637
Epoch 14 batch 50: loss=3.619208 grad_norm=44.0489 lr=1.000000e-04 gate_mean=0.5015 gate_entropy=0.6931
Epoch 14 batch 100: loss=2.380800 grad_norm=62.7712 lr=1.000000e-04 gate_mean=0.5020 gate_entropy=0.6931
Epoch 14 batch 150: loss=1.078186 grad_norm=40.6495 lr=1.000000e-04 gate_mean=0.5024 gate_entropy=0.6931
Epoch 14 batch 200: loss=2.037139 grad_norm=41.0627 lr=1.000000e-04 gate_mean=0.5019 gate_entropy=0.6931
Epoch 14 batch 250: loss=1.289384 grad_norm=93.2277 lr=1.000000e-04 gate_mean=0.5014 gate_entropy=0.6931
Epoch 14 batch 300: loss=0.746800 grad_norm=53.8658 lr=1.000000e-04 gate_mean=0.5020 gate_entropy=0.6931
Epoch 14 batch 350: loss=0.099624 grad_norm=36.7548 lr=1.000000e-04 gate_mean=0.5015 gate_entropy=0.6931
Epoch 14 batch 400: loss=1.275126 grad_norm=111.2274 lr=1.000000e-04 gate_mean=0.5016 gate_entropy=0.6931
Epoch 14 batch 450: loss=0.413799 grad_norm=67.8192 lr=1.000000e-04 gate_mean=0.5013 gate_entropy=0.6931
Epoch 14 batch 500: loss=-0.345538 grad_norm=66.0356 lr=1.000000e-04 gate_mean=0.5013 gate_entropy=0.6931
Epoch 14 batch 550: loss=0.316589 grad_norm=82.4367 lr=1.000000e-04 gate_mean=0.5021 gate_entropy=0.6931
Epoch 14 batch 600: loss=-0.254466 grad_norm=39.2771 lr=1.000000e-04 gate_mean=0.5017 gate_entropy=0.6931
Epoch 14 batch 650: loss=0.020744 grad_norm=52.9971 lr=1.000000e-04 gate_mean=0.5006 gate_entropy=0.6931
Epoch 14 batch 700: loss=-0.447961 grad_norm=36.7765 lr=1.000000e-04 gate_mean=0.5013 gate_entropy=0.6931
Epoch 14 batch 750: loss=-0.432378 grad_norm=70.9901 lr=1.000000e-04 gate_mean=0.5011 gate_entropy=0.6931
Epoch 14 batch 800: loss=-1.520707 grad_norm=64.8797 lr=1.000000e-04 gate_mean=0.5012 gate_entropy=0.6931
Epoch 14/15 train_loss=0.765608 val_loss=1.044907 gate_mean=0.5014 gate_std=0.0004 gate_entropy=0.6931 val_mae_close=0.23825934529304504
Epoch 15 batch 50: loss=-0.820873 grad_norm=44.7483 lr=1.000000e-04 gate_mean=0.5015 gate_entropy=0.6931
Epoch 15 batch 100: loss=-1.648895 grad_norm=267.9662 lr=1.000000e-04 gate_mean=0.5013 gate_entropy=0.6931
Epoch 15 batch 150: loss=-1.919134 grad_norm=54.0583 lr=1.000000e-04 gate_mean=0.5025 gate_entropy=0.6931
Epoch 15 batch 200: loss=-1.598210 grad_norm=24.6841 lr=1.000000e-04 gate_mean=0.5008 gate_entropy=0.6931
Epoch 15 batch 250: loss=-2.525520 grad_norm=20.0907 lr=1.000000e-04 gate_mean=0.5010 gate_entropy=0.6931
Epoch 15 batch 300: loss=-2.170583 grad_norm=239.6360 lr=1.000000e-04 gate_mean=0.5011 gate_entropy=0.6931
Epoch 15 batch 350: loss=-2.721407 grad_norm=198.1565 lr=1.000000e-04 gate_mean=0.5013 gate_entropy=0.6931
Epoch 15 batch 400: loss=-2.782088 grad_norm=113.7598 lr=1.000000e-04 gate_mean=0.5015 gate_entropy=0.6931
Epoch 15 batch 450: loss=-2.571510 grad_norm=34.9067 lr=1.000000e-04 gate_mean=0.5017 gate_entropy=0.6931
Epoch 15 batch 500: loss=-3.554850 grad_norm=203.3307 lr=1.000000e-04 gate_mean=0.5018 gate_entropy=0.6931
Epoch 15 batch 550: loss=-2.995960 grad_norm=84.0791 lr=1.000000e-04 gate_mean=0.5013 gate_entropy=0.6931
Epoch 15 batch 600: loss=-3.293502 grad_norm=96.7168 lr=1.000000e-04 gate_mean=0.5011 gate_entropy=0.6931
Epoch 15 batch 650: loss=-4.134937 grad_norm=274.3814 lr=1.000000e-04 gate_mean=0.5019 gate_entropy=0.6931
Epoch 15 batch 700: loss=-3.781855 grad_norm=439.4236 lr=1.000000e-04 gate_mean=0.5012 gate_entropy=0.6931
Epoch 15 batch 750: loss=-4.110889 grad_norm=50.8362 lr=1.000000e-04 gate_mean=0.5011 gate_entropy=0.6931
Epoch 15 batch 800: loss=9.647299 grad_norm=687.7510 lr=1.000000e-04 gate_mean=0.5013 gate_entropy=0.6931
Epoch 15/15 train_loss=-2.438703 val_loss=-1.898939 gate_mean=0.5014 gate_std=0.0004 gate_entropy=0.6931 val_mae_close=0.2039002627134323
Training complete
Available Samples: 1627; Batch Size: 64
        C_true    C_pred    r_gate
mean -0.158266 -0.278929  0.501423
std   1.526081  1.292773  0.000004